{
  "name": "Algorithm name Detection in Computer science Papers",
  "tagline": "Algorithm name detection framework for Research Papers",
  "body": "## A Bit of on Introduction\r\nThis task was assigned to Team 41 (which is us) as a part of Major Project for the Information Retrieval and Extraction Course. This project was divided into three phases. In the first phase we were required to come up with a scope document and deliverables for the second and the third phase respectively. In the second and third phase, we had to actually code and then implement what we had proposed.\r\n\r\n#Implementation:\r\n\r\n### Converting pdf to text\r\n* **Input** : A research paper in the pdf format.\r\n\r\n* **Output** : Need to convert that pdf to text format.\r\n\r\n* **Processing** : Using PDFMiner\r\n\r\n---\r\n            pdf2txt.py -O myoutput -o myoutput/myfile.text -t text myfile.pdf\r\n\r\n            Usage:\r\n                  pdf2txt.py [options] filename.pdf\r\n\r\n            Options:     \r\n                  1) -o output file name\r\n\r\n    \t\t  2) -t output format (text/html/xml/tag[for Tagged PDFs])\r\n\r\n    \t\t  3) -O dirname (triggers extraction of images from PDF into directory)\r\n\r\n---\r\n\r\n### Named Entity Recognition\r\n\r\n* **Input:** Research paper in the text format.\r\n\r\n* **Output :** Noun phrases (NNPS and NNs)\r\n\r\n* **Processing :**\r\n\r\n1. Sentence tokenization\r\n1. Merging the divided words at the end of the line [ex: div - \\n ision]\r\n1. Removing the part before the Abstract and after the Reference.\r\n1. Find the citation sentences and extract them\r\n1. Do pos_tagging for those sentences.\r\n1. Now extract the NNPS and NN. combine the NNPS occurring adjacent to each other in a sentence.\r\n\r\n\r\n---\r\n    def get_ready():\r\n        print \"IT BEGINS!!\"\r\n    get_ready()\r\n\r\n---\r\n\r\n### Filtration of the Named Entities\t\r\n\r\n* **Input :** Named Entities with author names, University names, places.\r\n* **Output :** stemmed desired named entities using porter stemmer.\r\n* **Processing :** \r\n1. Designed the list of authors and universities and places.\r\n1. And compare the named entities with these lists and filter them.\r\n1. Search for the word algorithm or technique to give more weightage to that particular word as the probability of getting the algorithm name will be high in such sentences.\r\n1. Stem these remaining named entities using Porter Stemmer\r\n\r\n###Trained set of vectors \r\n\r\n1. Gathered the trained set of vectors for the stemmed named entities of some research papers in our field of work(model.dat).\r\n1. Used Gensim python module, to load the ‘model.dat’. Used gensim.models.Word2Vec that contains vectors corresponding to each word.\r\n1. Form a list of true positives[containing name of actual computer science algorithms] and false positives [most common noise components in each paper] manually by going through the named entities of some papers.\r\n1. Compare each named entity extracted from paper with these lists of TPs and FPs and find the similarity between them. If the similarity between a word and another word in TP is greater than a threshold value (0.4 considered in our case), classify it as the TP, otherwise FP.\r\n\r\n\r\n\r\n\r\n",
  "note": "Don't delete this file! It's used internally to help with page regeneration."
}