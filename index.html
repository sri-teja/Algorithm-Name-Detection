<!DOCTYPE html>
<html lang="en-us">
  <head>
    <meta charset="UTF-8">
    <title>Algo Name Extraction from research papers</title>
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <link rel="stylesheet" type="text/css" href="normalize.css" media="screen">
    <link href='https://fonts.googleapis.com/css?family=Open+Sans:400,700' rel='stylesheet' type='text/css'>
    <link rel="stylesheet" type="text/css" href="stylesheet.css" media="screen">
    <link rel="stylesheet" type="text/css" href="github-light.css" media="screen">
  </head>
  <body>
    <section class="page-header">
      <h1 class="project-name">Algorithm Name Extraction</h1>
      <h2 class="project-tagline">Extracting Unique Algo names from Computer Science Research Papers </h2>
      <a href="https://github.com/sri-teja/ire_project" class="btn">View on GitHub</a>
      <a href="https://github.com/sri-teja/ire_project/zipball/master" class="btn">Download .zip</a>
      <a href="https://github.com/sri-teja/ire_project/tarball/master" class="btn">Download .tar.gz</a>
    </section>

    <section class="main-content">
      <h1>
<a id="heading" class="anchor" href="#heading" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Algorithm Name Extraction from Research Paper</h1>
<h2>Aim of project</h2>
<p>Extraction of algorithm name from research papers allows the user of this project to list out the name of algorithms being discussed in the paper by analysing the contents of the research document.
This project would assist the users to find research papers specific to a subdomain without actually opening and reading each of them.</p>

<h2>
<a id="a-bit-of-on-introduction" class="anchor" href="#a-bit-of-on-introduction" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Introduction</h2>
<strong>
<p>Major Project for Information Retrieval and Extraction Course, IIIT Hyderabad.</p></strong>
<p>
We are provided with PAKDD dataset of computer science papers. We construct a set of Named Entities from each paper in the data corpus. Identify algorithm names in
individual research papers, using some modified Named Entity Recognition techniques and Word2Vec models.</p>
<hr>
<h2>Overview of the Procedure</h2>
<p> Each unit in our data corpus(paper publication) consists of large number of named entities like the author name,
name of university or organization that published the paper, name of algorithm, name of
technology, references related to the paper, etc. which are extracted as Named Entities for the paper.
All the information obtained in the set, other than the 'name of algorithm', are noise to our system.
By using word2vec models we identify the set of true positives and false positives for each unit in dataset and report to user the algorithm names.</p>
<p>Following are the set of steps used to achieve the task - </p>
<ul>
<li><strong>Converting PDF to text</strong></li>
<li><strong>Sentence Tokenization</strong></li>
<li><strong>Word Tokenization</strong></li>
<li><strong>POS Tagging and NER</strong></li>
<li><strong>Citations Linking & filtering of NER</strong></li>
<li><strong>Word2Vec to find Algorithm Names</strong></li>
</ul>

<h2>Tools and Technology Used</h2>
<p>Some python libraries were used to implement the project. Following is the list of tools and tech used for each step - </p>
<ul>
<li><strong>PDF to Text Conversion using pdfminer library</strong>
<pre><code>
Usage: 
pdf2txt.py -O myoutput -o myoutput/myfile.text -t text myfile.pdf
pdf2txt.py [options] filename.pdf

Options:    -o output file name
            -t output format (text/html/xml/tag[for Tagged PDFs])
            -O dirname (triggers extraction of images from PDF into directory)
</code></pre>
</li>
<li><strong>Using NLTK library to Tokenize text document</strong>
<br/>
Sentence Tokenization :
<pre><code>
from nltk.tokenize import sent_tokenize
sent_tokenize_list = sent_tokenize(text_doc_data)
</code></pre>
<br/>Word Tokenization :
<pre><code>
from nltk.tokenize import word_tokenize
word_tokenize_list = word_tokenize(sentence)
</code></pre>
<br/>
POS Tagging :
<pre><code>
from nltk import pos_tag
pos_tagged_word= pos_tag(word_tokenize_list)
</code></pre>
</li>
<li><strong>Citations Linking & filtering of NER</strong>
Using Regex to extract citation sentences, and NNPS and NN pos tags to form a set of named entities.
</li>
<li><strong>Trained Word2Vec model is made use of via gensim library</strong>
<pre><code>
from gensim import models
model = models.Word2Vec.load('./model3.dat')
</code></pre>
</li>
<li>Filtering out Algorithm Name using model.similarity</li>

</ul>
      <footer class="site-footer">
        <span class="site-footer-credits">This page was generated by <a href="https://pages.github.com">GitHub Pages</a> using the <a href="https://github.com/jasonlong/cayman-theme">Cayman theme</a> by <a href="https://twitter.com/jasonlong">Jason Long</a>.</span>
      </footer>

    </section>

  
  </body>
</html>
